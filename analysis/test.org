:PROPERTIES:
:header-args: :async yes :session rhsp
:END:

* Preamble
#+name: preamble
#+begin_src jupyter-python
%load_ext autoreload
%autoreload 2

import functools

import jax
from jax import random
from jax import numpy as jnp
import optax
import matplotlib.pyplot as plt
import numpy as np
import numpyro
from numpyro.distributions import *
from numpyro import handlers
from numpyro.infer import Predictive
import pandas as pd
import seaborn as sns

import rhs

jax.config.update('jax_platform_name', 'cpu')
#+end_src

#+RESULTS: preamble
:RESULTS:
:END:
#+RESULTS:
* Prior

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
reparam = None
# reparam = rhs.ReparamIG(dec=False)
# reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=0.01, c_df=5., c_scale=1.)
trainer = rhs.TrainerMCMC(conf)
#+end_src

#+RESULTS:
a

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
# reparam = None
# reparam = rhs.ReparamIG(dec=True)
reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=100., c_df=10., c_scale=10.)
trainer = rhs.TrainerMCMC(conf)
samples = numpyro.infer.Predictive(trainer.conf.model, num_samples=500, exclude_deterministic=False)(random.key(0))
samples.keys()
#+end_src

#+RESULTS:
: dict_keys(['c', 'c2_aux', 'coef', 'coef_dec', 'lambda', 'lambda_aux1', 'lambda_aux1_dec', 'lambda_aux2', 'lambda_aux2_dec', 'noise', 'tau', 'tau_aux1', 'tau_aux1_dec', 'tau_aux2', 'tau_aux2_dec', 'y'])

#+begin_src jupyter-python
sites = ['c', 'coef']
fig, axs = plt.subplots(ncols=len(sites))
for site, ax in zip(sites, axs):
    ax.hist(samples[site].flatten(), bins=30)
    ax.set(title=site)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c938740cb7d3e8e78d025d2e67a059dd80cc0b51.png]]

#+begin_src jupyter-python
x = samples['lambda'].flatten()
x = x[x < 10]
plt.hist(x, bins=50)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3cf83e00a8eee01c1e4aa1fef611dbdef7d1495f.png]]

* SVI

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
reparam = None
# reparam = rhs.ReparamIG(dec=False)
# reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=0.01, c_df=5., c_scale=1.)
trainer = rhs.TrainerMCMC(conf)
#+end_src

#+RESULTS:
a

#+begin_src jupyter-python
trainer.train(10000)
plt.plot(trainer.losses)
trainer.save('/tmp/model.pkl')
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a5ac00226818ac84c61b55390328177d125a27fd.png]]

asd

#+begin_src jupyter-python
dataset.plot_coeffs(trainer)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b5bcf219cd073d9139c32c2a52bed8022f39540c.png]]


#+begin_src jupyter-python
fig, axs = plt.subplots(ncols=2, figsize=(8, 3), width_ratios=[1,3], sharey=True, layout='tight')

axs[0].set(title='Tau')
d = trainer.posterior('tau')
xs = np.linspace(d.icdf(.001), d.icdf(.999), 100)
p = np.exp(d.log_prob(xs))
axs[0].plot(xs, p / p.max())

axs[1].set(title='Lambda')
d = trainer.posterior('lambda')
xs = np.linspace(d.icdf(.001).min(), d.icdf(.999).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[1].plot(xs, (p / p.max(0)))

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2458db70e0572badf77bdc210796f7a3a5eb5498.png]]

#+begin_src jupyter-python
prior = trainer.prior('c')
post = trainer.posterior('c')
xs = jnp.linspace(post.icdf(.0001), max(prior.icdf(.99), post.icdf(.99)), 300)
plt.plot(xs, jnp.exp(prior.log_prob(xs)), label='prior')
plt.plot(xs, jnp.exp(post.log_prob(xs)), label='posterior')
plt.legend()
plt.gca().set(title='c')
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ff3e5b0f22377c3e028954b77b9854225e4bf4bc.png]]


#+begin_src jupyter-python
fig, axs = plt.subplots(nrows=2)

axs[0].set(title='lambda_aux1')
d = trainer.posterior('lambda_aux1')
xs = np.linspace(d.icdf(.001).min(), d.icdf(.9).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[0].plot(xs, (p / p.max(0)))
axs[0].plot(xs, (p:=np.exp(InverseGamma(.5, 1.).log_prob(xs)))/p.max(), c='k', ls='--')

axs[1].set(title='lambda_aux2')
d = trainer.posterior('lambda_aux2')
prior = Gamma(.5, 1.)
xs = np.linspace(d.icdf(.001).min(), d.icdf(.9).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[1].plot(xs, (p / p.max(0)))
axs[1].plot(xs, (p:=np.exp(prior.log_prob(xs)))/p.max(), c='k', ls='--')

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/4a7b742c8c3531c83d8ecc3cbf9bcc53f8772dc3.png]]

#+header: :var sharex="col" sharex='False
#+begin_src jupyter-python
sites = ['aux1_dec', 'aux1', 'aux2_dec', 'aux2', 'a', 'mu']
fig, axs = plt.subplots(
    nrows=len(models), ncols=len(sites), sharex=sharex,
    figsize=(3*len(sites),2*len(models)), gridspec_kw=dict(wspace=0.2, hspace=0.2))

for i, model in enumerate(models.values()):
    axs[i, 0].set(ylabel=model.centering)
    s = model.sample_posterior()

    for j, site in enumerate(sites):
        ax = axs[i, j]
        ax.set_yticks([], [])
        if i == 0:
            ax.set(title=site)
        try:
            d = model.posterior(site)
        except:
            ax.set_xticks([], [])
            continue

        alpha = 1.
        match site, model.centering:
            case 'aux1', Centering.DEC1 | Centering.DEC:
                alpha = 0.5
            case 'aux2', Centering.DEC2 | Centering.DEC:
                alpha = 0.5
            case 'a', _ if not model.centering is Centering.BASE:
                alpha = 0.5
        
        x = jnp.linspace(d.icdf(.01), d.icdf(.99), 300)
        ax.hist(s[site], bins=30, alpha=alpha)
        ax.axvline(s[site].mean(), ls='--', alpha=alpha)
        ax.text(1.0, .7, f'{s[site].mean():.4f}',
                transform=ax.transAxes,
                ha='right', va='top')

        axt = ax.twinx()
        axt.margins(0.)
        axt.yaxis.set_visible(False)
        axt.plot(x, jnp.exp(d.log_prob(x)), c='deeppink', alpha=alpha)
        axt.axvline(d.mean, ls='--', c='deeppink', alpha=alpha)
        ax.text(1.0, .9, f'{d.mean:.4f}',
                transform=ax.transAxes,
                ha='right', va='top', c='deeppink')

        if site == 'a':
            ax.axvline(TRUE.scale, lw=2, c='k')
        if site == 'mu':
            ax.axvline(TRUE.mean, lw=2, c='k')

None
#+end_src
* Structured guide
** Load
#+call: preamble()

#+RESULTS:

s

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(D_per=2)
reparam = None
# reparam = rhs.ReparamIG(dec=True)
structure = rhs.GuidePairCond()
# structure = rhs.GuidePairMv()
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, structure=structure, coef_dec=False, tau_scale=dataset.tau_scale, c_df=5., c_scale=1.)
trainer = rhs.TrainerSVI(conf)
#+end_src

#+RESULTS:

** Sampling
Confirm correctness posterior of coef by comparing it to trace sampling.

#+begin_src jupyter-python
trainer.params['corrs.lambda_coef'] = jnp.full(conf.D, -.9)
d = 0

samples = trainer.sample_posterior(method='trace')
plt.scatter(samples['lambda'][:, d], samples['coef'][:, d], label='Trace')

samples = trainer.sample_posterior(method='posterior')
plt.scatter(samples['lambda'][:, d], samples['coef'][:, d], label='Posterior')

plt.legend()
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/44beda4e438928a25aaef81fda7a910dbf0192a3.png]]


** Conditional
#+header: :var corr=.99
#+begin_src jupyter-python
lambda_ = LogNormal(0., .1)
lambdas = lambda_.icdf(jnp.array([.2, .5, .8]))
coef = Normal(-1., .1)
loc, scale = structure._posterior_coef(jnp.exp(lambda_.loc), lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
coef_cond = Normal(loc, scale)
colors = ['steelblue', 'deeppink', 'green']

fig, axs = plt.subplots(nrows=3, figsize=(5, 6), layout='tight')

ax = axs[0]
ax.set(title='lambda')
xs = jnp.linspace(lambda_.icdf(.01), lambda_.icdf(.99), 300)
ax.plot(xs, jnp.exp(lambda_.log_prob(xs)), c='k')
for c, x in zip(colors, lambdas):
    ax.axvline(x, c=c, lw=2, ls='--')

ax = axs[1]
ax.set(title='Coef')
xs = jnp.linspace(coef.icdf(.01), coef.icdf(.99), 300)
ax.plot(xs, jnp.exp(coef.log_prob(xs)), c='k')

ax = axs[2]
ax.set(title='Coef | lambda')
xs = jnp.linspace(coef_cond.icdf(.01), coef_cond.icdf(.99), 300)
ax.plot(xs, (p:=jnp.exp(coef_cond.log_prob(xs)))/p.max(), c='k')
for c, l in zip(colors, lambdas):
    loc, scale = structure._posterior_coef(l, lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
    coef_cond_sample = Normal(loc, scale)
    xs = jnp.linspace(coef_cond_sample.icdf(.01), coef_cond_sample.icdf(.99), 300)
    ax.plot(xs, (p:=jnp.exp(coef_cond_sample.log_prob(xs)))/p.max(), c=c, ls='--')
ax.sharex(axs[1]) # Share with coef marginal
    
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bc59ff135b771e3252b18d99a02b4a168b266f15.png]]

asd

#+header: :var corr=.001 corr=.9
#+begin_src jupyter-python
lambda_ = LogNormal(0., .1)
coef = Normal(-1., .1)
key = random.key(0)

# Multivariate
cov = lambda_.scale * coef.scale * corr
loglambda_coef = MultivariateNormal(
    jnp.array([lambda_.loc, coef.loc]),
    jnp.array([[lambda_.scale**2, cov],
               [cov,              coef.scale**2]]))

key, subkey = random.split(key)
samples = loglambda_coef.sample(subkey, (100,))
mv_samples = pd.DataFrame({'lambda': jnp.exp(samples[:, 0]), 'coef': samples[:, 1], 'method': 'multivariate'})

# Conditional
# lambdas = lambda_.sample(random.key(0), (100,))
lambdas = mv_samples['lambda']
coefs = np.empty((100,))
key, *subkeys = random.split(key, 101)
for i, l in enumerate(lambdas):
    loc, scale = structure._posterior_coef(l, lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
    coef_cond = Normal(loc, scale)
    coefs[i] = coef_cond.sample(subkeys[i])
cond_samples = pd.DataFrame({'lambda': lambdas, 'coef': coefs, 'method': 'conditional'})

# Plot
samples = pd.concat([cond_samples, mv_samples])
# sns.scatterplot(data=samples, x='lambda', y='coef', hue='method')
g = sns.jointplot(data=samples, x='lambda', y='coef', hue='method') 
g.ax_joint.axhline(coef.mean, c='k', ls='--', alpha=.5)
g.ax_joint.axvline(lambda_.mean, c='k', ls='--', alpha=.5)

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a6bb0e44a3b0bf65f41edc07c3eb66d5300c76d3.png]]

* Pair cond vs mv
#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(D_per=2)
trainer_cond = rhs.TrainerSVI(rhs.Configuration(dataset.X, dataset.Y, None, structure=rhs.GuidePairCond(), coef_dec=False, tau_scale=dataset.tau_scale, c_df=5., c_scale=1.))
trainer_mv = rhs.TrainerSVI(rhs.Configuration(dataset.X, dataset.Y, None, structure=rhs.GuidePairMv(), coef_dec=False, tau_scale=dataset.tau_scale, c_df=5., c_scale=1.))
trainers = dict(cond=trainer_cond, mv=trainer_mv)
#+end_src

#+RESULTS:

** Correlation in samples
#+begin_src jupyter-python
fig, axs = plt.subplots(ncols=2)
d = 0
for ax, (name, trainer) in zip(axs, trainers.items()):
    trainer.params['corrs.lambda_coef'] = jnp.full(trainer.conf.D, -.9)
    samples = trainer.sample_posterior(method='trace')
    ax.scatter(samples['lambda'][:, d], samples['coef'][:, d])
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5f311af9706d1e02c18b828a363d80d56361c797.png]]


** Compare log_probs

#+begin_src jupyter-python
# Fix params of the lambda and coef distributions. In both cases they are
# parameterized the same way: decoupled loc and scale for both, and a corr
# parameter.
data = {**trainer_mv.params}
# Fix the sample sites. For the multivariate case need joint samples, for
# the conditional case they are seperate sites.
loglambda_coef = trainer_mv.trace_guide()['loglambda_coef']['value']
data['loglambda_coef'] = loglambda_coef
data['lambda'] = jnp.exp(loglambda_coef[:, 0])
data['coef'] = loglambda_coef[:, 1]

# Trace the guide substituted with the data and calculate log probs.
traces = {}
for name, trainer in trainers.items():
    g = handlers.substitute(trainer.conf.guide, data)
    _, trace_g = numpyro.infer.util.get_importance_trace(
        handlers.seed(trainer.conf.model, 0), handlers.seed(g, 0), [], {}, {})
    traces[name] = trace_g

# In the multivariate case we simply need the log_prob from the join
print('Multivariate:', traces['mv']['loglambda_coef']['log_prob'])

# In the conditional case we need to add up the log_probs.
# Note that lambda needs jacobian adjustment for log transform
marg_lp = traces['cond']['lambda']['log_prob'] + loglambda_coef[:, 0]
cond_lp = traces['cond']['coef']['log_prob']
print('Conditional:', marg_lp + cond_lp)
#+end_src

#+RESULTS:
: Multivariate: [0.9460509 2.0908525 3.5045884 3.591191  2.237097  3.455056 ]
: Conditional: [0.94605076 2.090853   3.5045884  3.5911913  2.237096   3.4550557 ]

* Paired cond correlated
#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(D_per=10, B_k=np.array([6., 6., 6.]))
conf = rhs.Configuration(dataset.X, dataset.Y,
                         # None,
                         # rhs.ReparamIG(True),
                         rhs.ReparamII(False, True),
                         # structure=rhs.GuideUnstructured(),
                         structure=rhs.GuidePairCond(),
                         # structure=rhs.GuidePairCondCorr(5),
                         coef_dec=True,
                         tau_scale=dataset.tau_scale, c_df=3., c_scale=30.)
if isinstance(conf.structure, rhs.GuidePairCondCorr) and conf.structure.low_rank_factor:
    conf.inits['coef_factor'] = HalfNormal(.001).sample(random.key(0), (conf.D, conf.structure.low_rank_factor))
# trainer = rhs.TrainerSVI(conf, optim=optax.adam(0.01), elbo=numpyro.infer.Trace_ELBO(10))
trainer = rhs.TrainerSVI(conf, optim=optax.adam(0.01), elbo=numpyro.infer.TraceMeanField_ELBO(10))
#+end_src

#+RESULTS:

** Train
#+header: :var iters=(* 1000 3)
#+begin_src jupyter-python
trainer.train(iters)
plt.plot(trainer.losses)
plt.suptitle(trainer.losses[-1])
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/49c2f664acb3a346ca3b168022caa6aaedbf3689.png]]

alsjd

#+begin_src jupyter-python
dataset.plot_coeffs(trainer)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/13853e5fad519c21a5bdba2312a2c09496d0eb0b.png]]

lsjadskj

#+header: :var v=.3
#+begin_src jupyter-python
if isinstance(conf.structure, rhs.GuidePairCondCorr):
    if conf.structure.low_rank_factor is None:
        corr = (x:=trainer.params['chols.coef'])@x.T
        sns.heatmap(corr, center=0, vmin=-v, vmax=v)
    else:
        t = trainer.trace_guide()
        cov = t['coef_joint']['fn'].covariance_matrix
        sns.heatmap(cov, center=0, vmin=-v, vmax=v)

None
#+end_src
#+RESULTS:
