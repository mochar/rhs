:PROPERTIES:
:header-args: :async yes :session rhsp
:END:

* Preamble
#+name: preamble
#+begin_src jupyter-python
%load_ext autoreload
%autoreload 2

import functools
from pprint import pprint

import jax
from jax import random
from jax import numpy as jnp
import optax
import matplotlib.pyplot as plt
import numpy as np
import numpyro
from numpyro.distributions import *
from numpyro import handlers
from numpyro.infer import Predictive
import pandas as pd
import seaborn as sns

import rhs

jax.config.update('jax_platform_name', 'cpu')
#+end_src

#+RESULTS: preamble
#+RESULTS:
* Prior

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
reparam = None
# reparam = rhs.ReparamIG(dec=False)
# reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=0.01, c_df=5., c_scale=1.)
trainer = rhs.TrainerMCMC(conf)
#+end_src

#+RESULTS:
a

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
# reparam = None
# reparam = rhs.ReparamIG(dec=True)
reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=100., c_df=10., c_scale=10.)
trainer = rhs.TrainerMCMC(conf)
samples = numpyro.infer.Predictive(trainer.conf.model, num_samples=500, exclude_deterministic=False)(random.key(0))
samples.keys()
#+end_src

#+RESULTS:
: dict_keys(['c', 'c2_aux', 'coef', 'coef_dec', 'lambda', 'lambda_aux1', 'lambda_aux1_dec', 'lambda_aux2', 'lambda_aux2_dec', 'noise', 'tau', 'tau_aux1', 'tau_aux1_dec', 'tau_aux2', 'tau_aux2_dec', 'y'])

#+begin_src jupyter-python
sites = ['c', 'coef']
fig, axs = plt.subplots(ncols=len(sites))
for site, ax in zip(sites, axs):
    ax.hist(samples[site].flatten(), bins=30)
    ax.set(title=site)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c938740cb7d3e8e78d025d2e67a059dd80cc0b51.png]]

#+begin_src jupyter-python
x = samples['lambda'].flatten()
x = x[x < 10]
plt.hist(x, bins=50)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3cf83e00a8eee01c1e4aa1fef611dbdef7d1495f.png]]

* SVI

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
reparam = None
# reparam = rhs.ReparamIG(dec=False)
# reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=0.01, c_df=5., c_scale=1.)
trainer = rhs.TrainerMCMC(conf)
#+end_src

#+RESULTS:
a

#+begin_src jupyter-python
trainer.train(10000)
plt.plot(trainer.losses)
trainer.save('/tmp/model.pkl')
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a5ac00226818ac84c61b55390328177d125a27fd.png]]

asd

#+begin_src jupyter-python
dataset.plot_coeffs(trainer)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b5bcf219cd073d9139c32c2a52bed8022f39540c.png]]


#+begin_src jupyter-python
fig, axs = plt.subplots(ncols=2, figsize=(8, 3), width_ratios=[1,3], sharey=True, layout='tight')

axs[0].set(title='Tau')
d = trainer.posterior('tau')
xs = np.linspace(d.icdf(.001), d.icdf(.999), 100)
p = np.exp(d.log_prob(xs))
axs[0].plot(xs, p / p.max())

axs[1].set(title='Lambda')
d = trainer.posterior('lambda')
xs = np.linspace(d.icdf(.001).min(), d.icdf(.999).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[1].plot(xs, (p / p.max(0)))

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2458db70e0572badf77bdc210796f7a3a5eb5498.png]]

#+begin_src jupyter-python
prior = trainer.prior('c')
post = trainer.posterior('c')
xs = jnp.linspace(post.icdf(.0001), max(prior.icdf(.99), post.icdf(.99)), 300)
plt.plot(xs, jnp.exp(prior.log_prob(xs)), label='prior')
plt.plot(xs, jnp.exp(post.log_prob(xs)), label='posterior')
plt.legend()
plt.gca().set(title='c')
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ff3e5b0f22377c3e028954b77b9854225e4bf4bc.png]]


#+begin_src jupyter-python
fig, axs = plt.subplots(nrows=2)

axs[0].set(title='lambda_aux1')
d = trainer.posterior('lambda_aux1')
xs = np.linspace(d.icdf(.001).min(), d.icdf(.9).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[0].plot(xs, (p / p.max(0)))
axs[0].plot(xs, (p:=np.exp(InverseGamma(.5, 1.).log_prob(xs)))/p.max(), c='k', ls='--')

axs[1].set(title='lambda_aux2')
d = trainer.posterior('lambda_aux2')
prior = Gamma(.5, 1.)
xs = np.linspace(d.icdf(.001).min(), d.icdf(.9).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[1].plot(xs, (p / p.max(0)))
axs[1].plot(xs, (p:=np.exp(prior.log_prob(xs)))/p.max(), c='k', ls='--')

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/4a7b742c8c3531c83d8ecc3cbf9bcc53f8772dc3.png]]

#+header: :var sharex="col" sharex='False
#+begin_src jupyter-python
sites = ['aux1_dec', 'aux1', 'aux2_dec', 'aux2', 'a', 'mu']
fig, axs = plt.subplots(
    nrows=len(models), ncols=len(sites), sharex=sharex,
    figsize=(3*len(sites),2*len(models)), gridspec_kw=dict(wspace=0.2, hspace=0.2))

for i, model in enumerate(models.values()):
    axs[i, 0].set(ylabel=model.centering)
    s = model.sample_posterior()

    for j, site in enumerate(sites):
        ax = axs[i, j]
        ax.set_yticks([], [])
        if i == 0:
            ax.set(title=site)
        try:
            d = model.posterior(site)
        except:
            ax.set_xticks([], [])
            continue

        alpha = 1.
        match site, model.centering:
            case 'aux1', Centering.DEC1 | Centering.DEC:
                alpha = 0.5
            case 'aux2', Centering.DEC2 | Centering.DEC:
                alpha = 0.5
            case 'a', _ if not model.centering is Centering.BASE:
                alpha = 0.5
        
        x = jnp.linspace(d.icdf(.01), d.icdf(.99), 300)
        ax.hist(s[site], bins=30, alpha=alpha)
        ax.axvline(s[site].mean(), ls='--', alpha=alpha)
        ax.text(1.0, .7, f'{s[site].mean():.4f}',
                transform=ax.transAxes,
                ha='right', va='top')

        axt = ax.twinx()
        axt.margins(0.)
        axt.yaxis.set_visible(False)
        axt.plot(x, jnp.exp(d.log_prob(x)), c='deeppink', alpha=alpha)
        axt.axvline(d.mean, ls='--', c='deeppink', alpha=alpha)
        ax.text(1.0, .9, f'{d.mean:.4f}',
                transform=ax.transAxes,
                ha='right', va='top', c='deeppink')

        if site == 'a':
            ax.axvline(TRUE.scale, lw=2, c='k')
        if site == 'mu':
            ax.axvline(TRUE.mean, lw=2, c='k')

None
#+end_src
* Structured guide
** Load
#+call: preamble()

#+RESULTS:

s

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(D_per=2)
reparam = None
# reparam = rhs.ReparamIG(dec=True)
structure = rhs.GuidePairCond()
# structure = rhs.GuidePairMv()
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, structure=structure, coef_dec=False, tau_scale=dataset.tau_scale, c_df=5., c_scale=1.)
trainer = rhs.TrainerSVI(conf)
#+end_src

#+RESULTS:

** Sampling
Confirm correctness posterior of coef by comparing it to trace sampling.

#+begin_src jupyter-python
trainer.params['corrs.lambda_coef'] = jnp.full(conf.D, -.9)
d = 0

samples = trainer.sample_posterior(method='trace')
plt.scatter(samples['lambda'][:, d], samples['coef'][:, d], label='Trace')

samples = trainer.sample_posterior(method='posterior')
plt.scatter(samples['lambda'][:, d], samples['coef'][:, d], label='Posterior')

plt.legend()
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/44beda4e438928a25aaef81fda7a910dbf0192a3.png]]


** Conditional
#+header: :var corr=.99
#+begin_src jupyter-python
lambda_ = LogNormal(0., .1)
lambdas = lambda_.icdf(jnp.array([.2, .5, .8]))
coef = Normal(-1., .1)
loc, scale = structure._posterior_coef(jnp.exp(lambda_.loc), lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
coef_cond = Normal(loc, scale)
colors = ['steelblue', 'deeppink', 'green']

fig, axs = plt.subplots(nrows=3, figsize=(5, 6), layout='tight')

ax = axs[0]
ax.set(title='lambda')
xs = jnp.linspace(lambda_.icdf(.01), lambda_.icdf(.99), 300)
ax.plot(xs, jnp.exp(lambda_.log_prob(xs)), c='k')
for c, x in zip(colors, lambdas):
    ax.axvline(x, c=c, lw=2, ls='--')

ax = axs[1]
ax.set(title='Coef')
xs = jnp.linspace(coef.icdf(.01), coef.icdf(.99), 300)
ax.plot(xs, jnp.exp(coef.log_prob(xs)), c='k')

ax = axs[2]
ax.set(title='Coef | lambda')
xs = jnp.linspace(coef_cond.icdf(.01), coef_cond.icdf(.99), 300)
ax.plot(xs, (p:=jnp.exp(coef_cond.log_prob(xs)))/p.max(), c='k')
for c, l in zip(colors, lambdas):
    loc, scale = structure._posterior_coef(l, lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
    coef_cond_sample = Normal(loc, scale)
    xs = jnp.linspace(coef_cond_sample.icdf(.01), coef_cond_sample.icdf(.99), 300)
    ax.plot(xs, (p:=jnp.exp(coef_cond_sample.log_prob(xs)))/p.max(), c=c, ls='--')
ax.sharex(axs[1]) # Share with coef marginal
    
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bc59ff135b771e3252b18d99a02b4a168b266f15.png]]

asd

#+header: :var corr=.001 corr=.9
#+begin_src jupyter-python
lambda_ = LogNormal(0., .1)
coef = Normal(-1., .1)
key = random.key(0)

# Multivariate
cov = lambda_.scale * coef.scale * corr
loglambda_coef = MultivariateNormal(
    jnp.array([lambda_.loc, coef.loc]),
    jnp.array([[lambda_.scale**2, cov],
               [cov,              coef.scale**2]]))

key, subkey = random.split(key)
samples = loglambda_coef.sample(subkey, (100,))
mv_samples = pd.DataFrame({'lambda': jnp.exp(samples[:, 0]), 'coef': samples[:, 1], 'method': 'multivariate'})

# Conditional
# lambdas = lambda_.sample(random.key(0), (100,))
lambdas = mv_samples['lambda']
coefs = np.empty((100,))
key, *subkeys = random.split(key, 101)
for i, l in enumerate(lambdas):
    loc, scale = structure._posterior_coef(l, lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
    coef_cond = Normal(loc, scale)
    coefs[i] = coef_cond.sample(subkeys[i])
cond_samples = pd.DataFrame({'lambda': lambdas, 'coef': coefs, 'method': 'conditional'})

# Plot
samples = pd.concat([cond_samples, mv_samples])
# sns.scatterplot(data=samples, x='lambda', y='coef', hue='method')
g = sns.jointplot(data=samples, x='lambda', y='coef', hue='method') 
g.ax_joint.axhline(coef.mean, c='k', ls='--', alpha=.5)
g.ax_joint.axvline(lambda_.mean, c='k', ls='--', alpha=.5)

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a6bb0e44a3b0bf65f41edc07c3eb66d5300c76d3.png]]

* Pair cond vs mv
#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(D_per=2)
trainer_cond = rhs.TrainerSVI(rhs.Configuration(dataset.X, dataset.Y, None, structure=rhs.GuidePairCond(), coef_dec=False, tau_scale=dataset.tau_scale, c_df=5., c_scale=1.))
trainer_mv = rhs.TrainerSVI(rhs.Configuration(dataset.X, dataset.Y, None, structure=rhs.GuidePairMv(), coef_dec=False, tau_scale=dataset.tau_scale, c_df=5., c_scale=1.))
trainers = dict(cond=trainer_cond, mv=trainer_mv)
#+end_src

#+RESULTS:

** Correlation in samples
#+begin_src jupyter-python
fig, axs = plt.subplots(ncols=2)
d = 0
for ax, (name, trainer) in zip(axs, trainers.items()):
    trainer.params['corrs.lambda_coef'] = jnp.full(trainer.conf.D, -.9)
    samples = trainer.sample_posterior(method='trace')
    ax.scatter(samples['lambda'][:, d], samples['coef'][:, d])
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5f311af9706d1e02c18b828a363d80d56361c797.png]]


** Compare log_probs

#+begin_src jupyter-python
# Fix params of the lambda and coef distributions. In both cases they are
# parameterized the same way: decoupled loc and scale for both, and a corr
# parameter.
data = {**trainer_mv.params}
# Fix the sample sites. For the multivariate case need joint samples, for
# the conditional case they are seperate sites.
loglambda_coef = trainer_mv.trace_guide()['loglambda_coef']['value']
data['loglambda_coef'] = loglambda_coef
data['lambda'] = jnp.exp(loglambda_coef[:, 0])
data['coef'] = loglambda_coef[:, 1]

# Trace the guide substituted with the data and calculate log probs.
traces = {}
for name, trainer in trainers.items():
    g = handlers.substitute(trainer.conf.guide, data)
    _, trace_g = numpyro.infer.util.get_importance_trace(
        handlers.seed(trainer.conf.model, 0), handlers.seed(g, 0), [], {}, {})
    traces[name] = trace_g

# In the multivariate case we simply need the log_prob from the join
print('Multivariate:', traces['mv']['loglambda_coef']['log_prob'])

# In the conditional case we need to add up the log_probs.
# Note that lambda needs jacobian adjustment for log transform
marg_lp = traces['cond']['lambda']['log_prob'] + loglambda_coef[:, 0]
cond_lp = traces['cond']['coef']['log_prob']
print('Conditional:', marg_lp + cond_lp)
#+end_src

#+RESULTS:
: Multivariate: [0.9460509 2.0908525 3.5045884 3.591191  2.237097  3.455056 ]
: Conditional: [0.94605076 2.090853   3.5045884  3.5911913  2.237096   3.4550557 ]

* Paired cond correlated
#+CALL: preamble()

#+RESULTS:

** Dataset
#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(
    D_per=30, B_k=np.array([3., 3., 3.]),
    correlated=.8,
    x_std=.5, group_std=1.)

# dataset = rhs.datasets.TestDataset(
    # D_per=2, B_k=np.array([3.]),
    # correlated=.8,
    # x_std=.1, group_std=1.)

sns.heatmap(dataset.X, center=0)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/298f46374c1cbacc29fce8581619db967e28f801.png]]

** Model and train
#+begin_src jupyter-python
conf = rhs.Configuration(dataset.X, dataset.Y,
                         # None,
                         # rhs.ReparamIG(True),
                         # rhs.ReparamII(False, True),
                         reparam_tau=rhs.ReparamIG(True),
                         reparam_lambda=rhs.ReparamIG(True),
                         
                         structure=rhs.GuideUnstructured(),
                         # structure=rhs.GuidePairMv(),
                         # structure=rhs.GuidePairCond(),
                         # structure=rhs.GuidePairCondCorr(3),
                         # structure=rhs.GuidePairCondCorr(),
                         # structure=rhs.GuideFullMatrix(),
                         
                         coef_dec=True,
                         # coef_dec=False,
                         noise_scale=1.,

                         tau_scale=dataset.tau_scale, 
                         # tau_scale=0.1,
                         # c_df=1., c_scale=30.)
                         c_df=3., c_scale=3.)
if isinstance(conf.structure, rhs.GuidePairCondCorr) and conf.structure.low_rank_factor:
    # s = 0.001
    s = 0.1
    conf.inits['coef_factor'] = HalfNormal(s).sample(random.key(0), (conf.D, conf.structure.low_rank_factor))

# conf.inits['locs.noise'] = -3.
# for p, init in conf.inits.items():
    # if p.startswith('locs.tau'):
        # conf.inits[p] = jnp.full_like(init, -3.)

# elbo = numpyro.infer.Trace_ELBO
elbo = numpyro.infer.TraceMeanField_ELBO
elbo = functools.partial(rhs.TrainerSVI.build_specialized_elbo, conf)

trainer = rhs.TrainerSVI(
    conf,
    optim=optax.adam(0.01),
    elbo=elbo(num_particles=10))
#+end_src

#+RESULTS:

*** Train
#+header: :var iters=(* 1000 3 3)
#+begin_src jupyter-python
trainer.train(iters)
plt.figure(figsize=(5,4))
plt.plot(trainer.losses)
plt.suptitle(trainer.losses[-1])
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b66004c3f6d47e4d75c294a8a26f8e6ecc156d95.png]]

alsjd

#+begin_src jupyter-python
dataset.plot_coeffs(trainer, True)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/9c43b8abd568254e0b8030d394696d6a4c4ef086.png]]

lsjadskj

#+header: :var v=.1
#+begin_src jupyter-python
if isinstance(conf.structure, rhs.GuidePairCondCorr):
    if conf.structure.low_rank_factor is None:
        corr = (x:=trainer.params['chols.coef'])@x.T
        sns.heatmap(corr, center=0, vmin=-v, vmax=v)
    else:
        t = trainer.trace_guide()
        cov = t['coef_joint']['fn'].covariance_matrix
        sns.heatmap(cov, center=0, vmin=-v, vmax=v)

None
#+end_src
#+RESULTS:

** Lambda
#+header: :var sharex="col" sharex='False threshold=5. 
#+begin_src jupyter-python :eval no
sites = ['lambda_aux1_dec', 'lambda_aux1', 'lambda_aux2_dec', 'lambda_aux2', 'lambda', 'coef_dec', 'coef']
fig, axs = plt.subplots(
    nrows=dataset.K, ncols=len(sites), sharex=sharex,
    figsize=(3*len(sites),3*dataset.K),
    gridspec_kw=dict(wspace=0.2, hspace=0.2))

s = trainer.sample_posterior()

for k in range(dataset.K):
    m = trainer.posterior('lambda').mode
    m = m[k*dataset.D_per:(k+1)*dataset.D_per]
    ind = m.argmax() + k*dataset.D_per
    
    for j, site in enumerate(sites):
        ax = axs[k, j]
        ax.set_yticks([], [])
        if k == 0:
            ax.set(title=site)
        try:
            d = trainer.posterior(site)
            d = d.__class__(d.loc[ind], d.scale[ind])
        except:
            ax.set_xticks([], [])
            continue
    
        alpha = 1.  
        if (site.endswith('aux1') and getattr(trainer.conf.reparam, 'dec1', False)) or \
           (site.endswith('aux2') and (getattr(trainer.conf.reparam, 'dec2', False) or getattr(trainer.conf.reparam, 'dec', False))) or \
           (site == 'lambda' and trainer.conf.reparam) or \
           (site == 'coef'):
            alpha = 0.5
    
        x = jnp.linspace(d.icdf(.01), d.icdf(.99), 300)
        ss = s[site][:, ind]
        ax.hist(ss, bins=30, alpha=alpha)
        ax.axvline(ss.mean(), ls='--', alpha=alpha)
        ax.text(1.0, .7, f'{ss.mean():.4f}',
                transform=ax.transAxes,
                ha='right', va='top')
    
        axt = ax.twinx()
        axt.margins(0.)
        axt.yaxis.set_visible(False)
        axt.plot(x, jnp.exp(d.log_prob(x)), c='deeppink', alpha=alpha)
        axt.axvline(d.mean, ls='--', c='deeppink', alpha=alpha)
        ax.text(1.0, .9, f'{d.mean:.4f}',
                transform=ax.transAxes,
                ha='right', va='top', c='deeppink')
    
        if site == 'a':
            ax.axvline(TRUE.scale, lw=2, c='k')
        if site == 'coef':
            ax.axvline(dataset.B_k_std[k], lw=2, c='k')
    
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3cb964adfa140986174bf7ff33ba988e9e155e79.png]]


* Dependency structures
** Model
#+begin_src jupyter-python
from numpyro.infer.inspect import get_dependencies

conf = rhs.Configuration(jnp.zeros((2,2)), jnp.zeros(2), None, rhs.GuideUnstructured(), False, .1, 1., 1.)
dependencies = get_dependencies(conf.model)
pprint(dependencies)
#+end_src

#+RESULTS:
#+begin_example
{'posterior_dependencies': {'c2_aux': {'c2_aux': set(),
                                       'coef': set(),
                                       'lambda': set(),
                                       'tau': set()},
                            'coef': {'coef': {'d'}, 'y': set()},
                            'lambda': {'coef': set(), 'lambda': set()},
                            'noise': {'coef': set(),
                                      'noise': set(),
                                      'y': set()},
                            'tau': {'coef': set(),
                                    'lambda': set(),
                                    'tau': set()}},
 'prior_dependencies': {'c2_aux': {'c2_aux': set()},
                        'coef': {'c2_aux': set(),
                                 'coef': set(),
                                 'lambda': set(),
                                 'tau': set()},
                        'lambda': {'lambda': set()},
                        'noise': {'noise': set()},
                        'tau': {'tau': set()},
                        'y': {'coef': set(), 'noise': set(), 'y': set()}}}
#+end_example

** Guide
#+begin_src jupyter-python
from numpyro.infer.inspect import get_model_relations, get_dependencies

guide_structures = [rhs.GuideUnstructured(), rhs.GuidePairMv(), rhs.GuidePairCond(), rhs.GuidePairCondCorr(), rhs.GuidePairCondCorr(3)]#, rhs.GuideFullMatrix()]
for structure in guide_structures:
    conf = rhs.Configuration(jnp.zeros((2,2)), jnp.zeros(2), None, structure, False, .1, 1., 1.)
    dependencies = get_dependencies(conf.guide)
    print('='*3, structure, '='*3)
    print('Prior:')
    pprint(dependencies['prior_dependencies'], indent=4)
    print('Posterior:')
    pprint(dependencies['posterior_dependencies'], indent=4)
    print('\n\n')
#+end_src

#+RESULTS:
#+begin_example
=== GuideUnstructured() ===
Prior:
{   'c2_aux': {'c2_aux': set()},
    'coef': {'coef': set()},
    'lambda': {'lambda': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}
Posterior:
{   'c2_aux': {'c2_aux': set()},
    'coef': {'coef': set()},
    'lambda': {'lambda': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}



=== GuidePairMv() ===
Prior:
{   'c2_aux': {'c2_aux': set()},
    'loglambda_coef': {'loglambda_coef': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}
Posterior:
{   'c2_aux': {'c2_aux': set()},
    'loglambda_coef': {'loglambda_coef': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}



=== GuidePairCond() ===
Prior:
{   'c2_aux': {'c2_aux': set()},
    'coef': {'coef': set(), 'lambda': set()},
    'lambda': {'lambda': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}
Posterior:
{   'c2_aux': {'c2_aux': set()},
    'coef': {'coef': set()},
    'lambda': {'coef': set(), 'lambda': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}



=== GuidePairCondCorr(low_rank_factor=None) ===
Prior:
{   'c2_aux': {'c2_aux': set()},
    'coef_joint': {'coef_joint': set(), 'lambda': set()},
    'lambda': {'lambda': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}
Posterior:
{   'c2_aux': {'c2_aux': set()},
    'coef_joint': {'coef_joint': set()},
    'lambda': {'coef_joint': set(), 'lambda': {'d'}},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}



=== GuidePairCondCorr(low_rank_factor=3) ===
Prior:
{   'c2_aux': {'c2_aux': set()},
    'coef_joint': {'coef_joint': set(), 'lambda': set()},
    'lambda': {'lambda': set()},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}
Posterior:
{   'c2_aux': {'c2_aux': set()},
    'coef_joint': {'coef_joint': set()},
    'lambda': {'coef_joint': set(), 'lambda': {'d'}},
    'noise': {'noise': set()},
    'tau': {'tau': set()}}
#+end_example

* Simplified
+ Only one group with two features
+ Better see the multimodal posterior

** Dataset
#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(
    D_per=2, B_k=np.array([3.]),
    correlated=.8,
    x_std=.1, group_std=1.)

sns.heatmap(dataset.X, center=0)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/f0aa544ebeb9cf1a102bcef094de809b77b60a21.png]]

** MCMC
#+begin_src jupyter-python
conf = rhs.Configuration(dataset.X, dataset.Y,
                         # None,
                         # rhs.ReparamIG(True),
                         # rhs.ReparamII(False, True),
                         reparam_tau=rhs.ReparamII(True, True),
                         reparam_lambda=rhs.ReparamII(True, True),
                         structure=rhs.GuideUnstructured(),
                         coef_dec=True,
                         noise_scale=1.,
                         tau_scale=dataset.tau_scale, 
                         c_df=3., c_scale=30.)
# trainer = rhs.TrainerMCMC(conf, num_warmup=1000, num_samples=1000)
trainer = rhs.TrainerMCMC(conf, num_warmup=1000, num_samples=1000)
trainer.train()
print('Diverging:', trainer.diverging.sum())
#+end_src

#+RESULTS:
: sample: 100%|███████| 2000/2000 [00:03<00:00, 629.98it/s, 127 steps of size 2.29e-02. acc. prob=0.90]Diverging: 26
:

Colinearity between the two coefficients.
#+begin_src jupyter-python
plt.scatter(*trainer.samples['coef'].T)
plt.scatter(*trainer.samples['coef'][trainer.diverging].T, c='red')
plt.axhline(dataset.B_k_std, c='red')
plt.axvline(dataset.B_k_std, c='red')
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/666b3aae0e4abaa5e109e93a1cd12bd6001b6842.png]]

Correlation between lambda and coefficient.
#+begin_src jupyter-python
fig, axs = plt.subplots(ncols=2, figsize=(8,3), sharex=True, sharey=True)
for i, ax in enumerate(axs):
    for diverging in [False, True]:
        sub = trainer.diverging ^ (not diverging)
        ax.scatter(trainer.samples['lambda'][sub, i],
                   trainer.samples['coef_dec'][sub, i],
                   c='red' if diverging else 'steelblue')
    ax.set(xlabel='Lambda', ylabel='Coef')
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/1d2514bf2eac974530fcc573b229a3414e400b42.png]]
