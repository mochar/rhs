:PROPERTIES:
:header-args: :async yes :session rhsp
:END:

* Preamble
#+name: preamble
#+begin_src jupyter-python
%load_ext autoreload
%autoreload 2

import functools

import jax
from jax import random
from jax import numpy as jnp
import matplotlib.pyplot as plt
import numpy as np
import numpyro
from numpyro.distributions import *
from numpyro.infer import Predictive
import pandas as pd
import seaborn as sns

import rhs

jax.config.update('jax_platform_name', 'cpu')
#+end_src

#+RESULTS: preamble

#+RESULTS:
* Prior

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
reparam = None
# reparam = rhs.ReparamIG(dec=False)
# reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=0.01, c_df=5., c_scale=1.)
trainer = rhs.TrainerMCMC(conf)
#+end_src

#+RESULTS:
a

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
# reparam = None
# reparam = rhs.ReparamIG(dec=True)
reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=100., c_df=10., c_scale=10.)
trainer = rhs.TrainerMCMC(conf)
samples = numpyro.infer.Predictive(trainer.conf.model, num_samples=500, exclude_deterministic=False)(random.key(0))
samples.keys()
#+end_src

#+RESULTS:
: dict_keys(['c', 'c2_aux', 'coef', 'coef_dec', 'lambda', 'lambda_aux1', 'lambda_aux1_dec', 'lambda_aux2', 'lambda_aux2_dec', 'noise', 'tau', 'tau_aux1', 'tau_aux1_dec', 'tau_aux2', 'tau_aux2_dec', 'y'])

#+begin_src jupyter-python
sites = ['c', 'coef']
fig, axs = plt.subplots(ncols=len(sites))
for site, ax in zip(sites, axs):
    ax.hist(samples[site].flatten(), bins=30)
    ax.set(title=site)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c938740cb7d3e8e78d025d2e67a059dd80cc0b51.png]]

#+begin_src jupyter-python
x = samples['lambda'].flatten()
x = x[x < 10]
plt.hist(x, bins=50)
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3cf83e00a8eee01c1e4aa1fef611dbdef7d1495f.png]]

* SVI

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset()
reparam = None
# reparam = rhs.ReparamIG(dec=False)
# reparam = rhs.ReparamII(dec1=True, dec2=True)
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, tau_scale=0.01, c_df=5., c_scale=1.)
trainer = rhs.TrainerMCMC(conf)
#+end_src

#+RESULTS:
a

#+begin_src jupyter-python
trainer.train(10000)
plt.plot(trainer.losses)
trainer.save('/tmp/model.pkl')
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a5ac00226818ac84c61b55390328177d125a27fd.png]]

asd

#+begin_src jupyter-python
dataset.plot_coeffs(trainer)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b5bcf219cd073d9139c32c2a52bed8022f39540c.png]]


#+begin_src jupyter-python
fig, axs = plt.subplots(ncols=2, figsize=(8, 3), width_ratios=[1,3], sharey=True, layout='tight')

axs[0].set(title='Tau')
d = trainer.posterior('tau')
xs = np.linspace(d.icdf(.001), d.icdf(.999), 100)
p = np.exp(d.log_prob(xs))
axs[0].plot(xs, p / p.max())

axs[1].set(title='Lambda')
d = trainer.posterior('lambda')
xs = np.linspace(d.icdf(.001).min(), d.icdf(.999).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[1].plot(xs, (p / p.max(0)))

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2458db70e0572badf77bdc210796f7a3a5eb5498.png]]

#+begin_src jupyter-python
prior = trainer.prior('c')
post = trainer.posterior('c')
xs = jnp.linspace(post.icdf(.0001), max(prior.icdf(.99), post.icdf(.99)), 300)
plt.plot(xs, jnp.exp(prior.log_prob(xs)), label='prior')
plt.plot(xs, jnp.exp(post.log_prob(xs)), label='posterior')
plt.legend()
plt.gca().set(title='c')
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ff3e5b0f22377c3e028954b77b9854225e4bf4bc.png]]


#+begin_src jupyter-python
fig, axs = plt.subplots(nrows=2)

axs[0].set(title='lambda_aux1')
d = trainer.posterior('lambda_aux1')
xs = np.linspace(d.icdf(.001).min(), d.icdf(.9).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[0].plot(xs, (p / p.max(0)))
axs[0].plot(xs, (p:=np.exp(InverseGamma(.5, 1.).log_prob(xs)))/p.max(), c='k', ls='--')

axs[1].set(title='lambda_aux2')
d = trainer.posterior('lambda_aux2')
prior = Gamma(.5, 1.)
xs = np.linspace(d.icdf(.001).min(), d.icdf(.9).max(), 100)
p = np.exp(d.log_prob(xs[:, None]))
axs[1].plot(xs, (p / p.max(0)))
axs[1].plot(xs, (p:=np.exp(prior.log_prob(xs)))/p.max(), c='k', ls='--')

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/4a7b742c8c3531c83d8ecc3cbf9bcc53f8772dc3.png]]

#+header: :var sharex="col" sharex='False
#+begin_src jupyter-python
sites = ['aux1_dec', 'aux1', 'aux2_dec', 'aux2', 'a', 'mu']
fig, axs = plt.subplots(
    nrows=len(models), ncols=len(sites), sharex=sharex,
    figsize=(3*len(sites),2*len(models)), gridspec_kw=dict(wspace=0.2, hspace=0.2))

for i, model in enumerate(models.values()):
    axs[i, 0].set(ylabel=model.centering)
    s = model.sample_posterior()

    for j, site in enumerate(sites):
        ax = axs[i, j]
        ax.set_yticks([], [])
        if i == 0:
            ax.set(title=site)
        try:
            d = model.posterior(site)
        except:
            ax.set_xticks([], [])
            continue

        alpha = 1.
        match site, model.centering:
            case 'aux1', Centering.DEC1 | Centering.DEC:
                alpha = 0.5
            case 'aux2', Centering.DEC2 | Centering.DEC:
                alpha = 0.5
            case 'a', _ if not model.centering is Centering.BASE:
                alpha = 0.5
        
        x = jnp.linspace(d.icdf(.01), d.icdf(.99), 300)
        ax.hist(s[site], bins=30, alpha=alpha)
        ax.axvline(s[site].mean(), ls='--', alpha=alpha)
        ax.text(1.0, .7, f'{s[site].mean():.4f}',
                transform=ax.transAxes,
                ha='right', va='top')

        axt = ax.twinx()
        axt.margins(0.)
        axt.yaxis.set_visible(False)
        axt.plot(x, jnp.exp(d.log_prob(x)), c='deeppink', alpha=alpha)
        axt.axvline(d.mean, ls='--', c='deeppink', alpha=alpha)
        ax.text(1.0, .9, f'{d.mean:.4f}',
                transform=ax.transAxes,
                ha='right', va='top', c='deeppink')

        if site == 'a':
            ax.axvline(TRUE.scale, lw=2, c='k')
        if site == 'mu':
            ax.axvline(TRUE.mean, lw=2, c='k')

None
#+end_src
* Structured guide
** Load
#+call: preamble()

#+RESULTS:

#+begin_src jupyter-python
dataset = rhs.datasets.TestDataset(D_per=2)
reparam = None
# reparam = rhs.ReparamIG(dec=True)
structure = rhs.GuideStructured()
conf = rhs.Configuration(dataset.X, dataset.Y, reparam, structure=structure, coef_dec=False, tau_scale=dataset.tau_scale, c_df=5., c_scale=1.)
trainer = rhs.TrainerSVI(conf)
#+end_src

#+RESULTS:

** Sampling
Confirm correctness posterior of coef by comparing it to trace sampling.

#+begin_src jupyter-python
trainer.params['corrs.coef_lambda'] = jnp.full(conf.D, -.9)
d = 0

samples = trainer.sample_posterior(method='trace')
plt.scatter(samples['lambda'][:, d], samples['coef'][:, d], label='Trace')

samples = trainer.sample_posterior(method='posterior')
plt.scatter(samples['lambda'][:, d], samples['coef'][:, d], label='Posterior')

plt.legend()
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/56a17d47348eb9f2620abf52a1eda3ef76ce4fb0.png]]


** Conditional
#+header: :var corr=.99
#+begin_src jupyter-python
lambda_ = LogNormal(0., .1)
lambdas = lambda_.icdf(jnp.array([.2, .5, .8]))
coef = Normal(-1., .1)
loc, scale = structure._posterior_coef(jnp.exp(lambda_.loc), lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
coef_cond = Normal(loc, scale)
colors = ['steelblue', 'deeppink', 'green']

fig, axs = plt.subplots(nrows=3, figsize=(5, 6), layout='tight')

ax = axs[0]
ax.set(title='lambda')
xs = jnp.linspace(lambda_.icdf(.01), lambda_.icdf(.99), 300)
ax.plot(xs, jnp.exp(lambda_.log_prob(xs)), c='k')
for c, x in zip(colors, lambdas):
    ax.axvline(x, c=c, lw=2, ls='--')

ax = axs[1]
ax.set(title='Coef')
xs = jnp.linspace(coef.icdf(.01), coef.icdf(.99), 300)
ax.plot(xs, jnp.exp(coef.log_prob(xs)), c='k')

ax = axs[2]
ax.set(title='Coef | lambda')
xs = jnp.linspace(coef_cond.icdf(.01), coef_cond.icdf(.99), 300)
ax.plot(xs, (p:=jnp.exp(coef_cond.log_prob(xs)))/p.max(), c='k')
for c, l in zip(colors, lambdas):
    loc, scale = structure._posterior_coef(l, lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
    coef_cond_sample = Normal(loc, scale)
    xs = jnp.linspace(coef_cond_sample.icdf(.01), coef_cond_sample.icdf(.99), 300)
    ax.plot(xs, (p:=jnp.exp(coef_cond_sample.log_prob(xs)))/p.max(), c=c, ls='--')
ax.sharex(axs[1]) # Share with coef marginal
    
None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bc59ff135b771e3252b18d99a02b4a168b266f15.png]]

asd

#+header: :var corr=.001 corr=.9
#+begin_src jupyter-python
lambda_ = LogNormal(0., .1)
coef = Normal(-1., .1)
key = random.key(0)

# Multivariate
cov = lambda_.scale * coef.scale * corr
loglambda_coef = MultivariateNormal(
    jnp.array([lambda_.loc, coef.loc]),
    jnp.array([[lambda_.scale**2, cov],
               [cov,              coef.scale**2]]))

key, subkey = random.split(key)
samples = loglambda_coef.sample(subkey, (100,))
mv_samples = pd.DataFrame({'lambda': jnp.exp(samples[:, 0]), 'coef': samples[:, 1], 'method': 'multivariate'})

# Conditional
# lambdas = lambda_.sample(random.key(0), (100,))
lambdas = mv_samples['lambda']
coefs = np.empty((100,))
key, *subkeys = random.split(key, 101)
for i, l in enumerate(lambdas):
    loc, scale = structure._posterior_coef(l, lambda_.loc, lambda_.scale, coef.loc, coef.scale, corr)
    coef_cond = Normal(loc, scale)
    coefs[i] = coef_cond.sample(subkeys[i])
cond_samples = pd.DataFrame({'lambda': lambdas, 'coef': coefs, 'method': 'conditional'})

# Plot
samples = pd.concat([cond_samples, mv_samples])
# sns.scatterplot(data=samples, x='lambda', y='coef', hue='method')
g = sns.jointplot(data=samples, x='lambda', y='coef', hue='method') 
g.ax_joint.axhline(coef.mean, c='k', ls='--', alpha=.5)
g.ax_joint.axvline(lambda_.mean, c='k', ls='--', alpha=.5)

None
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a6bb0e44a3b0bf65f41edc07c3eb66d5300c76d3.png]]
