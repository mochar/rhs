#+bibliography: ~/ilm/main/refs.bib
#+CITATION_STYLE: csl nature.csl
# #+cite_export: csl nature.csl
#+OPTIONS: toc:nil

Sparsifying priors such as the horseshoe are normally used when there are many features, but only some are predictive. What happens when most features are predictive, but also redundant? Ideally we'd like a sparse solution; a minimal number of features have a non-zero coefficient. [cite:@piironen_projective_2020]

To explore this I implement regularized horseshoe priors in numpyro and train it with variational inference and MCMC with different variations of the model:

- Guide structure :: Model correlations between coefficient and its local sparsity parameter
- Reparamerization :: Factorize half-cauchy prior into gamma and inverse-gamma distributions

The resutls are discussed on my website.

* Reparameterizations
The global and feature-local sparsity parameters are modeled as half-Cauchy distributions in the prior. Standard exponential family variational approximations struggle to capture the thick Cauchy tails, and using Cauchy approximating family leads to high variance gradients. This can challenge inference and is therefore proposed to be factorized into Gamma and inverse-Gamma distributions.

In variational inference, the advantage is that the KL-divergence between a (inverse) gamma and a log-normal distribution is closed-form. The log-normal distribution can therefore be used as a variational posterior leading to lower variance gradients. In Pyro, we can use ~MeanFieldELBO~ to use the closed-form solution.

** InverseGamma-Gamma 
The square of half-Cauchy \(\mathcal{C}^+\) is equal in distribution to a product of Gamma and inverse-Gamma. [cite:@oh_radial_2019] [cite:@louizos_bayesian_2017]

If \(z \sim \mathcal{C}^+(k)\), then 

\begin{align*}
a &\sim \text{Gamma}(\frac{1}{2},k^2) \\
b &\sim \text{InverseGamma}(\frac{1}{2},1) \\
\sqrt{z} &= ab
\end{align*}

** InverseGamma-InverseGamma
The square of half-Cauchy \(\mathcal{C}^+\) is a result of sampling successively from two inverse-Gamma distributions. [cite:@neville_mean_2014] [cite:@ghosh_model_2019]

If \(z \sim \mathcal{C}^+(k)\), then 

\begin{align*}
a &\sim \text{InvGamma}(\frac{1}{2}, \frac{1}{k^2}) \\
z^2 | a &\sim \text{InvGamma}(\frac{1}{2}, \frac{1}{a})
\end{align*}


* Guide structures 
There exists a tight coupling between the coefficient and its horseshoe determined variance. Mean-field variational posterior will fail to capture this covariance. [cite:@louizos_bayesian_2017] Therefore different structured guide have been implemented:

- Paired multivariate normal :: Model each coeffcient-lambda pair as a multivariate normal.
\begin{align*}
(\hat{\lambda}_d, \theta_d) \sim \text{MvNormal}()
\end{align*}
  Note that \(\hat{\lambda} = \ln \lambda\) as \(\lambda\) is positive.
- Paired conditional multivariate normal :: Same but in [[https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions][conditional form]].
\begin{align*}
\lambda_d &\sim \text{LogNormal}() \\ 
\theta_d | \lambda_d &\sim \text{Normal}()
\end{align*}
  Since \(\lambda\) is now univariate, we can calculate its KL-divergence directly to its prior using the gamma reparameterization.
- Paired correlated :: ???
- Full :: Model the entire coefficient-lambda pair matrix with a matrix normal.

* References
#+PRINT_BIBLIOGRAPHY:
